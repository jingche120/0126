{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingche120/0126/blob/main/%E3%80%90Demo07c%E3%80%91AI%E4%BB%A3%E7%90%86%E8%A8%AD%E8%A8%88%E6%A8%A1%E5%BC%8F_%E5%93%A1%E7%91%9B%E5%BC%8F%E6%80%9D%E8%80%83%E7%94%9F%E6%88%90%E5%99%A8Two_Stage_CoT%E7%89%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ¯ ä»»å‹™èªªæ˜ï¼šå°‡ã€Œå¤šæ…§å¼æ€è€ƒç”¢ç”Ÿå™¨ã€æ”¹å¯«ç‚º Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†éç¨‹\n",
        "\n",
        "**åŸå§‹ä»»å‹™ï¼š** è¼¸å…¥ä¸€ä»¶å°äº‹æˆ–å€’æ¥£äº‹ï¼Œç”Ÿæˆä¸€æ®µè¶…ç´šæ­£å‘çš„ç¤¾ç¾¤è²¼æ–‡ã€‚\n",
        "\n",
        "#### âœ… CoT æ”¹å¯«ç‰ˆæœ¬æµç¨‹ï¼š\n",
        "1. **ç¬¬ä¸€éšæ®µï¼ˆæ€è€ƒéšæ®µï¼‰**ï¼šè«‹ LLM æ€è€ƒäº”ç¨®ã€Œç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…ç´šå¹¸é‹çš„äº‹ã€çš„åŸå› \n",
        "2. **ç¬¬äºŒéšæ®µï¼ˆç”¢æ–‡éšæ®µï¼‰**ï¼šå¾é€™äº”å€‹åŸå› ä¸­æŒ‘å‡ºæœ€æœ‰è¶£çš„ä¸€å€‹ï¼Œå¯«æˆç¤¾ç¾¤ç™¼æ–‡ï¼ˆå“¡ç‘›å¼é¢¨æ ¼ + ç¬¬ä¸€äººç¨± + emoji + \"å®Œå…¨æ˜¯ Lucky Vicky å‘€!\" çµå°¾ï¼‰\n",
        "\n",
        "é€™å°±æ˜¯å…¸å‹çš„ Planning æ¨¡å¼æ‡‰ç”¨ï¼šå…ˆæ‹†è§£ã€å¾ŒåŸ·è¡Œã€‚"
      ],
      "metadata": {
        "id": "3PE0ExrCbGFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. è®€å…¥ä½ çš„é‡‘é‘°\n",
        "\n",
        "è«‹ä¾ä½ ä½¿ç”¨çš„æœå‹™, æ±ºå®šè®€å…¥å“ªå€‹é‡‘é‘°"
      ],
      "metadata": {
        "id": "7YnD5hfb5sO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "mGOx_1226Mjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ã€ä½¿ç”¨ Mistralã€‘\n",
        "# api_key = userdata.get('Mistral')\n",
        "# os.environ['MISTRAL']=api_key\n",
        "# provider = \"mistral\"\n",
        "# model = \"ministral-8b-latest\"\n",
        "\n",
        "#ã€ä½¿ç”¨ OpenAIã€‘\n",
        "# api_key = userdata.get('OpenAI')\n",
        "# os.environ['OPENAI_API_KEY']=api_key\n",
        "# provider = \"openai\"\n",
        "# model = \"gpt-4o\"\n",
        "\n",
        "#ã€ä½¿ç”¨ Groqã€‘\n",
        "api_key = userdata.get('Groq')\n",
        "os.environ['GROQ_API_KEY']=api_key\n",
        "provider = \"groq\"\n",
        "model = \"openai/gpt-oss-120b\""
      ],
      "metadata": {
        "id": "L507G1B65rPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aisuite[all]"
      ],
      "metadata": {
        "id": "_290bzOow2f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. ä½¿ç”¨ AISuite çš„æº–å‚™"
      ],
      "metadata": {
        "id": "gEJPZBwB_6YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai"
      ],
      "metadata": {
        "id": "weMgIhwVJQ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "provider_planner = \"groq\"\n",
        "model_planner=\"openai/gpt-oss-120b\"\n",
        "\n",
        "provider_writer = \"groq\"\n",
        "model_writer = \"openai/gpt-oss-120b\"\n",
        "\n",
        "# provider_planner = \"groq\"\n",
        "# model_planner = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "# provider_writer = \"groq\"\n",
        "# model_writer = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "#provider_reviewer = \"openai\"\n",
        "#model_reviewer = \"gpt-4o\""
      ],
      "metadata": {
        "id": "2rCILaUN_8Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reply(system=\"è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚\",\n",
        "          prompt=\"Hi\",\n",
        "          provider=\"groq\",\n",
        "          model=\"openai/gpt-oss-120b\"\n",
        "          ):\n",
        "\n",
        "    client = ai.Client()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(model=f\"{provider}:{model}\", messages=messages)\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "jzrCV9IDkW5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  3. æ‰“é€ äºŒéšæ®µ"
      ],
      "metadata": {
        "id": "uZ96aYDCbthW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_planner = \"è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›æ‡‰: ä½ æ˜¯ä¸€ä½æ­£å‘æ€è€ƒçš„å¤§å¸«, æ“…é•·å°‡é è¨€æœªä¾†ã€‚è«‹é‡å°ä½¿ç”¨è€…çš„äº‹ä»¶ï¼Œæ€è€ƒå‡ºäº”ç¨®ã€æœªä¾†çš„ç”¢å“ã€çš„åŸå› , åˆ—å‡ºæ¢åˆ—å¼æ¸…å–®ã€‚ç”¨ç¬¬ä¸€äººç¨±æ€è€ƒä¹Ÿå¯ä»¥ã€‚\"\n",
        "system_writer = \"è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›æ‡‰: ä½ æ˜¯ä¸€ä½è¶…ç´šæ¨‚è§€çš„ç¤¾ç¾¤è²¼æ–‡å°å¤©ä½¿ï¼Œé¢¨æ ¼æ´»æ½‘å¥½ç¬‘ï¼Œå–„æ–¼ç”¨æ­£å‘è§€é»æ’°å¯«è²¼æ–‡ã€‚è«‹æ ¹æ“šä¸€å€‹ç‰¹åˆ¥æœ‰è¶£çš„ç†ç”±ï¼Œå¯«å‡ºä¸€æ®µ Instagram ç™¼æ–‡å£å»çš„ç¬¬ä¸€äººç¨±è²¼æ–‡ã€‚çµå°¾ä¸€å®šè¦èªªï¼šã€æœªä¾†æ˜¯å……æ»¿å¸Œæœ›çš„å‘€ï¼ã€è€Œä¸”ä¸­é–“è¦åŠ å…¥ emojiã€‚\""
      ],
      "metadata": {
        "id": "vHXKzIWEVWAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LGB7dbtZU89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lucky_post(prompt):\n",
        "    # Step 1: CoT - æ€è€ƒäº”å€‹æœªä¾†çš„ç”¢å“\n",
        "    planning_prompt = f\"ä½¿ç”¨è€…èªªï¼š{prompt}ã€‚è«‹å¹«æˆ‘æƒ³äº”å€‹æœªä¾†çš„ç”¢å“ã€‚\"\n",
        "    lucky_reasons = reply(system_planner, planning_prompt,\n",
        "                          provider = provider_planner,\n",
        "                          model = model_planner\n",
        "                          )\n",
        "\n",
        "    # Step 2: é¸å‡ºæœ€æœ‰è¶£ä¸€é …ï¼Œå¯«æˆè²¼æ–‡\n",
        "    generation_prompt = f\"é€™æ˜¯æˆ‘æƒ³åˆ°çš„äº”å€‹ç”¢å“ï¼š\\n{lucky_reasons}\\n\\nè«‹å¾ä¸­é¸å‡ºä¸€å€‹æœ€æœ‰è¶£çš„ï¼Œç„¶å¾Œæ ¹æ“šå®ƒå¯«ä¸€æ®µå¤šæ…§å¼ç™¼æ–‡ã€‚\"\n",
        "    final_post = reply(system_writer, generation_prompt,\n",
        "                       provider = provider_writer,\n",
        "                       model = model_writer\n",
        "                       )\n",
        "\n",
        "    return lucky_reasons, final_post"
      ],
      "metadata": {
        "id": "pOKBaJ2vb5vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ç”¨ Gradio æ‰“é€ ä½ çš„å°è©±æ©Ÿå™¨äºº Web App!"
      ],
      "metadata": {
        "id": "WuzrvcdQx2VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "IFsBQr0-y62k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "xv7nvjfDj3f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### ğŸ€ å¤šæ…§å¼æ€è€ƒç”¢ç”Ÿå™¨ Lucky Vicky ğŸŒˆ\")\n",
        "    gr.Markdown(\"è«‹è¼¸å…¥ä¸€ä»¶ä»Šå¤©ç™¼ç”Ÿçš„äº‹è«‹ï¼Œè®“ AI å¹«ä½ æƒ³å‡ºé»å­ï¼\")\n",
        "    user_input = gr.Textbox(label=\"ä»Šå¤©ç™¼ç”Ÿçš„äº‹æƒ…æ˜¯â€¦\")\n",
        "    btn = gr.Button(\"ç”Ÿæˆ Lucky Vicky çš„ç¥ç´šè²¼æ–‡ âœ¨\")\n",
        "\n",
        "    with gr.Row():\n",
        "        out1 = gr.Textbox(label=\"ğŸ§  äº”å€‹è¶…å¹¸é‹çš„ç†ç”±ï¼ˆPlanning CoTï¼‰\")\n",
        "        out2 = gr.Textbox(label=\"ğŸ“£ æœ€çµ‚å¤šæ…§å¼è²¼æ–‡ï¼ˆPo æ–‡ï¼‰\")\n",
        "\n",
        "    btn.click(lucky_post, inputs=[user_input], outputs=[out1, out2])"
      ],
      "metadata": {
        "id": "YkivsZv--eR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "r9vqHkbrdldw",
        "outputId": "00b5ef2c-a347-466e-c3c9-769ffb2ac408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6aeda3e77cf88e9d75.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6aeda3e77cf88e9d75.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-469513221.py\", line 4, in lucky_post\n",
            "    lucky_reasons = reply(system_planner, planning_prompt,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1967477355.py\", line 15, in reply\n",
            "    response = client.chat.completions.create(model=f\"{provider}:{model}\", messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aisuite/client.py\", line 221, in create\n",
            "    self.client.providers[provider_key] = ProviderFactory.create_provider(\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aisuite/provider.py\", line 46, in create_provider\n",
            "    return provider_class(**config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aisuite/providers/groq_provider.py\", line 44, in __init__\n",
            "    self.client = groq.Groq(**config)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_client.py\", line 99, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 824, in __init__\n",
            "    self._client = http_client or SyncHttpxClientWrapper(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 722, in __init__\n",
            "    super().__init__(**kwargs)\n",
            "TypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-469513221.py\", line 4, in lucky_post\n",
            "    lucky_reasons = reply(system_planner, planning_prompt,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1967477355.py\", line 15, in reply\n",
            "    response = client.chat.completions.create(model=f\"{provider}:{model}\", messages=messages)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aisuite/client.py\", line 221, in create\n",
            "    self.client.providers[provider_key] = ProviderFactory.create_provider(\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aisuite/provider.py\", line 46, in create_provider\n",
            "    return provider_class(**config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/aisuite/providers/groq_provider.py\", line 44, in __init__\n",
            "    self.client = groq.Groq(**config)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_client.py\", line 99, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 824, in __init__\n",
            "    self._client = http_client or SyncHttpxClientWrapper(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\", line 722, in __init__\n",
            "    super().__init__(**kwargs)\n",
            "TypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n"
          ]
        }
      ]
    }
  ]
}